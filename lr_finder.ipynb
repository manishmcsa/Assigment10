{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lr_finder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUaJd97AZyobXN3f9GQnCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishmcsa/Assigment10/blob/main/lr_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grREYkO9WJqH"
      },
      "source": [
        "from __future__ import print_function, with_statement, division\r\n",
        "import copy\r\n",
        "import os\r\n",
        "import torch\r\n",
        "from tqdm.autonotebook import tqdm\r\n",
        "from torch.optim.lr_scheduler import _LRScheduler\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glTlyp92WMDm",
        "outputId": "c5ef7021-4cfb-4119-e071-f5df7b9f2deb"
      },
      "source": [
        "try:\r\n",
        "    from apex import amp\r\n",
        "\r\n",
        "    IS_AMP_AVAILABLE = True\r\n",
        "except ImportError:\r\n",
        "    import logging\r\n",
        "\r\n",
        "    logging.basicConfig()\r\n",
        "    logger = logging.getLogger(__name__)\r\n",
        "    logger.warning(\r\n",
        "        \"To enable mixed precision training, please install `apex`. \"\r\n",
        "        \"Or you can re-install this package by the following command:\\n\"\r\n",
        "        '  pip install torch-lr-finder -v --global-option=\"amp\"'\r\n",
        "    )\r\n",
        "    IS_AMP_AVAILABLE = False\r\n",
        "    del logging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:__main__:To enable mixed precision training, please install `apex`. Or you can re-install this package by the following command:\n",
            "  pip install torch-lr-finder -v --global-option=\"amp\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO9Zu8iLWaPx"
      },
      "source": [
        "class LRFinder(object):\r\n",
        "    \"\"\"Learning rate range test.\r\n",
        "    The learning rate range test increases the learning rate in a pre-training run\r\n",
        "    between two boundaries in a linear or exponential manner. It provides valuable\r\n",
        "    information on how well the network can be trained over a range of learning rates\r\n",
        "    and what is the optimal learning rate.\r\n",
        "    Arguments:\r\n",
        "        model (torch.nn.Module): wrapped model.\r\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\r\n",
        "            is assumed to be the lower boundary of the range test.\r\n",
        "        criterion (torch.nn.Module): wrapped loss function.\r\n",
        "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\r\n",
        "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\r\n",
        "            Alternatively, can be an object representing the device on which the\r\n",
        "            computation will take place. Default: None, uses the same device as `model`.\r\n",
        "        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\r\n",
        "            model and optimizer will be cached in memory. Otherwise, they will be saved\r\n",
        "            to files under the `cache_dir`.\r\n",
        "        cache_dir (string, optional): path for storing temporary files. If no path is\r\n",
        "            specified, system-wide temporary directory is used. Notice that this\r\n",
        "            parameter will be ignored if `memory_cache` is True.\r\n",
        "    Example:\r\n",
        "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\r\n",
        "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\r\n",
        "        >>> lr_finder.plot() # to inspect the loss-learning rate graph\r\n",
        "        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\r\n",
        "    Reference:\r\n",
        "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\r\n",
        "    fastai/lr_find: https://github.com/fastai/fastai\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        model,\r\n",
        "        optimizer,\r\n",
        "        criterion,\r\n",
        "        device=None,\r\n",
        "        memory_cache=True,\r\n",
        "        cache_dir=None,\r\n",
        "    ):\r\n",
        "        # Check if the optimizer is already attached to a scheduler\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self._check_for_scheduler()\r\n",
        "\r\n",
        "        self.model = model\r\n",
        "        self.criterion = criterion\r\n",
        "        self.history = {\"lr\": [], \"loss\": []}\r\n",
        "        self.best_loss = None\r\n",
        "        self.memory_cache = memory_cache\r\n",
        "        self.cache_dir = cache_dir\r\n",
        "\r\n",
        "        # Save the original state of the model and optimizer so they can be restored if\r\n",
        "        # needed\r\n",
        "        self.model_device = next(self.model.parameters()).device\r\n",
        "        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\r\n",
        "        self.state_cacher.store(\"model\", self.model.state_dict())\r\n",
        "        self.state_cacher.store(\"optimizer\", self.optimizer.state_dict())\r\n",
        "\r\n",
        "        # If device is None, use the same as the model\r\n",
        "        if device:\r\n",
        "            self.device = device\r\n",
        "        else:\r\n",
        "            self.device = self.model_device\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\r\n",
        "\r\n",
        "        self.model.load_state_dict(self.state_cacher.retrieve(\"model\"))\r\n",
        "        self.optimizer.load_state_dict(self.state_cacher.retrieve(\"optimizer\"))\r\n",
        "        self.model.to(self.model_device)\r\n",
        "\r\n",
        "    def range_test(\r\n",
        "        self,\r\n",
        "        train_loader,\r\n",
        "        val_loader=None,\r\n",
        "        start_lr=None,\r\n",
        "        end_lr=10,\r\n",
        "        num_iter=100,\r\n",
        "        step_mode=\"exp\",\r\n",
        "        smooth_f=0.05,\r\n",
        "        diverge_th=5,\r\n",
        "        accumulation_steps=1,\r\n",
        "    ):\r\n",
        "        \"\"\"Performs the learning rate range test.\r\n",
        "        Arguments:\r\n",
        "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\r\n",
        "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\r\n",
        "                will only use the training loss. When given a data loader, the model is\r\n",
        "                evaluated after each iteration on that dataset and the evaluation loss\r\n",
        "                is used. Note that in this mode the test takes significantly longer but\r\n",
        "                generally produces more precise results. Default: None.\r\n",
        "            start_lr (float, optional): the starting learning rate for the range test.\r\n",
        "                Default: None (uses the learning rate from the optimizer).\r\n",
        "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\r\n",
        "            num_iter (int, optional): the number of iterations over which the test\r\n",
        "                occurs. Default: 100.\r\n",
        "            step_mode (str, optional): one of the available learning rate policies,\r\n",
        "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\r\n",
        "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\r\n",
        "                interval. Disabled if set to 0, otherwise the loss is smoothed using\r\n",
        "                exponential smoothing. Default: 0.05.\r\n",
        "            diverge_th (int, optional): the test is stopped when the loss surpasses the\r\n",
        "                threshold:  diverge_th * best_loss. Default: 5.\r\n",
        "            accumulation_steps (int, optional): steps for gradient accumulation. If it\r\n",
        "                is 1, gradients are not accumulated. Default: 1.\r\n",
        "        Example (fastai approach):\r\n",
        "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\r\n",
        "            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\r\n",
        "        Example (Leslie Smith's approach):\r\n",
        "            >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\r\n",
        "            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=\"linear\")\r\n",
        "        Gradient accumulation is supported; example:\r\n",
        "            >>> train_data = ...    # prepared dataset\r\n",
        "            >>> desired_bs, real_bs = 32, 4         # batch size\r\n",
        "            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\r\n",
        "            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\r\n",
        "            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\r\n",
        "            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\r\n",
        "        Reference:\r\n",
        "        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\r\n",
        "        https://medium.com/huggingface/ec88c3e51255)\r\n",
        "        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # Reset test results\r\n",
        "        self.history = {\"lr\": [], \"loss\": []}\r\n",
        "        self.best_loss = None\r\n",
        "\r\n",
        "        # Move the model to the proper device\r\n",
        "        self.model.to(self.device)\r\n",
        "\r\n",
        "        # Check if the optimizer is already attached to a scheduler\r\n",
        "        self._check_for_scheduler()\r\n",
        "\r\n",
        "        # Set the starting learning rate\r\n",
        "        if start_lr:\r\n",
        "            self._set_learning_rate(start_lr)\r\n",
        "\r\n",
        "        # Initialize the proper learning rate policy\r\n",
        "        if step_mode.lower() == \"exp\":\r\n",
        "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\r\n",
        "        elif step_mode.lower() == \"linear\":\r\n",
        "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\r\n",
        "        else:\r\n",
        "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\r\n",
        "\r\n",
        "        if smooth_f < 0 or smooth_f >= 1:\r\n",
        "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\r\n",
        "\r\n",
        "        # Create an iterator to get data batch by batch\r\n",
        "        iter_wrapper = DataLoaderIterWrapper(train_loader)\r\n",
        "        for iteration in tqdm(range(num_iter)):\r\n",
        "            # Train on batch and retrieve loss\r\n",
        "            loss = self._train_batch(iter_wrapper, accumulation_steps)\r\n",
        "            if val_loader:\r\n",
        "                loss = self._validate(val_loader)\r\n",
        "\r\n",
        "            # Update the learning rate\r\n",
        "            lr_schedule.step()\r\n",
        "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\r\n",
        "\r\n",
        "            # Track the best loss and smooth it if smooth_f is specified\r\n",
        "            if iteration == 0:\r\n",
        "                self.best_loss = loss\r\n",
        "            else:\r\n",
        "                if smooth_f > 0:\r\n",
        "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\r\n",
        "                if loss < self.best_loss:\r\n",
        "                    self.best_loss = loss\r\n",
        "\r\n",
        "            # Check if the loss has diverged; if it has, stop the test\r\n",
        "            self.history[\"loss\"].append(loss)\r\n",
        "            if loss > diverge_th * self.best_loss:\r\n",
        "                print(\"Stopping early, the loss has diverged\")\r\n",
        "                break\r\n",
        "\r\n",
        "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\r\n",
        "\r\n",
        "    def _set_learning_rate(self, new_lrs):\r\n",
        "        if not isinstance(new_lrs, list):\r\n",
        "            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\r\n",
        "        if len(new_lrs) != len(self.optimizer.param_groups):\r\n",
        "            raise ValueError(\r\n",
        "                \"Length of `new_lrs` is not equal to the number of parameter groups \"\r\n",
        "                + \"in the given optimizer\"\r\n",
        "            )\r\n",
        "\r\n",
        "        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\r\n",
        "            param_group[\"lr\"] = new_lr\r\n",
        "\r\n",
        "    def _check_for_scheduler(self):\r\n",
        "        for param_group in self.optimizer.param_groups:\r\n",
        "            if \"initial_lr\" in param_group:\r\n",
        "                raise RuntimeError(\"Optimizer already has a scheduler attached to it\")\r\n",
        "\r\n",
        "    def _train_batch(self, iter_wrapper, accumulation_steps):\r\n",
        "        self.model.train()\r\n",
        "        total_loss = None  # for late initialization\r\n",
        "\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        for i in range(accumulation_steps):\r\n",
        "            inputs, labels = iter_wrapper.get_batch()\r\n",
        "            inputs, labels = self._move_to_device(inputs, labels)\r\n",
        "\r\n",
        "            # Forward pass\r\n",
        "            outputs = self.model(inputs)\r\n",
        "            loss = self.criterion(outputs, labels)\r\n",
        "\r\n",
        "            # Loss should be averaged in each step\r\n",
        "            loss /= accumulation_steps\r\n",
        "\r\n",
        "            # Backward pass\r\n",
        "            if IS_AMP_AVAILABLE and hasattr(self.optimizer, \"_amp_stash\"):\r\n",
        "                # For minor performance optimization, see also:\r\n",
        "                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\r\n",
        "                delay_unscale = ((i + 1) % accumulation_steps) != 0\r\n",
        "\r\n",
        "                with amp.scale_loss(\r\n",
        "                    loss, self.optimizer, delay_unscale=delay_unscale\r\n",
        "                ) as scaled_loss:\r\n",
        "                    scaled_loss.backward()\r\n",
        "            else:\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "            if total_loss is None:\r\n",
        "                total_loss = loss\r\n",
        "            else:\r\n",
        "                total_loss += loss\r\n",
        "\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        return total_loss.item()\r\n",
        "\r\n",
        "    def _move_to_device(self, inputs, labels):\r\n",
        "        def move(obj, device):\r\n",
        "            if isinstance(obj, tuple):\r\n",
        "                return tuple(move(o, device) for o in obj)\r\n",
        "            elif torch.is_tensor(obj):\r\n",
        "                return obj.to(device)\r\n",
        "            elif isinstance(obj, list):\r\n",
        "                return [move(o, device) for o in obj]\r\n",
        "            else:\r\n",
        "                return obj\r\n",
        "\r\n",
        "        inputs = move(inputs, self.device)\r\n",
        "        labels = move(labels, self.device)\r\n",
        "        return inputs, labels\r\n",
        "\r\n",
        "    def _validate(self, dataloader):\r\n",
        "        # Set model to evaluation mode and disable gradient computation\r\n",
        "        running_loss = 0\r\n",
        "        self.model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            for inputs, labels in dataloader:\r\n",
        "                # Move data to the correct device\r\n",
        "                inputs, labels = self._move_to_device(inputs, labels)\r\n",
        "\r\n",
        "                if isinstance(inputs, tuple) or isinstance(inputs, list):\r\n",
        "                    batch_size = inputs[0].size(0)\r\n",
        "                else:\r\n",
        "                    batch_size = inputs.size(0)\r\n",
        "\r\n",
        "                # Forward pass and loss computation\r\n",
        "                outputs = self.model(inputs)\r\n",
        "                loss = self.criterion(outputs, labels)\r\n",
        "                running_loss += loss.item() * batch_size\r\n",
        "\r\n",
        "        return running_loss / len(dataloader.dataset)\r\n",
        "\r\n",
        "    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\r\n",
        "        \"\"\"Plots the learning rate range test.\r\n",
        "        Arguments:\r\n",
        "            skip_start (int, optional): number of batches to trim from the start.\r\n",
        "                Default: 10.\r\n",
        "            skip_end (int, optional): number of batches to trim from the start.\r\n",
        "                Default: 5.\r\n",
        "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\r\n",
        "                scale; otherwise, plotted in a linear scale. Default: True.\r\n",
        "            show_lr (float, optional): if set, adds a vertical line to visualize the\r\n",
        "                specified learning rate. Default: None.\r\n",
        "            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\r\n",
        "                matplotlib axes object and the figure is not be shown. If `None`, then\r\n",
        "                the figure and axes object are created in this method and the figure is\r\n",
        "                shown . Default: None.\r\n",
        "        Returns:\r\n",
        "            The matplotlib.axes.Axes object that contains the plot.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if skip_start < 0:\r\n",
        "            raise ValueError(\"skip_start cannot be negative\")\r\n",
        "        if skip_end < 0:\r\n",
        "            raise ValueError(\"skip_end cannot be negative\")\r\n",
        "        if show_lr is not None and not isinstance(show_lr, float):\r\n",
        "            raise ValueError(\"show_lr must be float\")\r\n",
        "\r\n",
        "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\r\n",
        "        # properly so the behaviour is the expected\r\n",
        "        lrs = self.history[\"lr\"]\r\n",
        "        losses = self.history[\"loss\"]\r\n",
        "        if skip_end == 0:\r\n",
        "            lrs = lrs[skip_start:]\r\n",
        "            losses = losses[skip_start:]\r\n",
        "        else:\r\n",
        "            lrs = lrs[skip_start:-skip_end]\r\n",
        "            losses = losses[skip_start:-skip_end]\r\n",
        "\r\n",
        "        # Create the figure and axes object if axes was not already given\r\n",
        "        fig = None\r\n",
        "        if ax is None:\r\n",
        "            fig, ax = plt.subplots()\r\n",
        "\r\n",
        "        # Plot loss as a function of the learning rate\r\n",
        "        ax.plot(lrs, losses)\r\n",
        "        if log_lr:\r\n",
        "            ax.set_xscale(\"log\")\r\n",
        "        ax.set_xlabel(\"Learning rate\")\r\n",
        "        ax.set_ylabel(\"Loss\")\r\n",
        "\r\n",
        "        if show_lr is not None:\r\n",
        "            ax.axvline(x=show_lr, color=\"red\")\r\n",
        "\r\n",
        "        # Show only if the figure was created internally\r\n",
        "        if fig is not None:\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "        return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im9cRKkNWbcu"
      },
      "source": [
        "class ExponentialLR(_LRScheduler):\r\n",
        "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\r\n",
        "    iterations.\r\n",
        "    Arguments:\r\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer.\r\n",
        "        end_lr (float): the final learning rate.\r\n",
        "        num_iter (int): the number of iterations over which the test occurs.\r\n",
        "        last_epoch (int, optional): the index of last epoch. Default: -1.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\r\n",
        "        self.end_lr = end_lr\r\n",
        "        self.num_iter = num_iter\r\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\r\n",
        "\r\n",
        "    def get_lr(self):\r\n",
        "        curr_iter = self.last_epoch + 1\r\n",
        "        r = curr_iter / self.num_iter\r\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F973ZgYHWmQ2"
      },
      "source": [
        "class StateCacher(object):\r\n",
        "    def __init__(self, in_memory, cache_dir=None):\r\n",
        "        self.in_memory = in_memory\r\n",
        "        self.cache_dir = cache_dir\r\n",
        "\r\n",
        "        if self.cache_dir is None:\r\n",
        "            import tempfile\r\n",
        "\r\n",
        "            self.cache_dir = tempfile.gettempdir()\r\n",
        "        else:\r\n",
        "            if not os.path.isdir(self.cache_dir):\r\n",
        "                raise ValueError(\"Given `cache_dir` is not a valid directory.\")\r\n",
        "\r\n",
        "        self.cached = {}\r\n",
        "\r\n",
        "    def store(self, key, state_dict):\r\n",
        "        if self.in_memory:\r\n",
        "            self.cached.update({key: copy.deepcopy(state_dict)})\r\n",
        "        else:\r\n",
        "            fn = os.path.join(self.cache_dir, \"state_{}_{}.pt\".format(key, id(self)))\r\n",
        "            self.cached.update({key: fn})\r\n",
        "            torch.save(state_dict, fn)\r\n",
        "\r\n",
        "    def retrieve(self, key):\r\n",
        "        if key not in self.cached:\r\n",
        "            raise KeyError(\"Target {} was not cached.\".format(key))\r\n",
        "\r\n",
        "        if self.in_memory:\r\n",
        "            return self.cached.get(key)\r\n",
        "        else:\r\n",
        "            fn = self.cached.get(key)\r\n",
        "            if not os.path.exists(fn):\r\n",
        "                raise RuntimeError(\r\n",
        "                    \"Failed to load state in {}. File doesn't exist anymore.\".format(fn)\r\n",
        "                )\r\n",
        "            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\r\n",
        "            return state_dict\r\n",
        "\r\n",
        "    def __del__(self):\r\n",
        "        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\r\n",
        "        this instance being destroyed.\"\"\"\r\n",
        "\r\n",
        "        if self.in_memory:\r\n",
        "            return\r\n",
        "\r\n",
        "        for k in self.cached:\r\n",
        "            if os.path.exists(self.cached[k]):\r\n",
        "                os.remove(self.cached[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zqGnGEtWr3e"
      },
      "source": [
        "class DataLoaderIterWrapper(object):\r\n",
        "    \"\"\"A wrapper for iterating `torch.utils.data.DataLoader` with the ability to reset\r\n",
        "    itself while `StopIteration` is raised.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, data_loader, auto_reset=True):\r\n",
        "        self.data_loader = data_loader\r\n",
        "        self.auto_reset = auto_reset\r\n",
        "        self._iterator = iter(data_loader)\r\n",
        "\r\n",
        "    def __next__(self):\r\n",
        "        # Get a new set of inputs and labels\r\n",
        "        try:\r\n",
        "            inputs, labels = next(self._iterator)\r\n",
        "        except StopIteration:\r\n",
        "            if not self.auto_reset:\r\n",
        "                raise\r\n",
        "            self._iterator = iter(self.data_loader)\r\n",
        "            inputs, labels, *_ = next(self._iterator)\r\n",
        "\r\n",
        "        return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUG4Xn7dW2Id"
      },
      "source": [
        "def get_batch(self):\r\n",
        "    return next(self)"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}