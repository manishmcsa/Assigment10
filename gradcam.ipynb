{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradcam.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJ9mewxI5Hq7NKbOYSxqH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manishmcsa/Assigment10/blob/main/gradcam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoQZ-SPV1R9z"
      },
      "source": [
        "import torch\r\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX0ylsuu1VEa"
      },
      "source": [
        "class GradCam(object):\r\n",
        "\r\n",
        "\tdef __init__(self, model, target_layers, num_classes):\r\n",
        "\t\tsuper(GradCam, self).__init__()\r\n",
        "\t\tself.model = model\r\n",
        "\t\tself.target_layers = target_layers\r\n",
        "\t\tself.num_classes = num_classes\r\n",
        "\t\tself.device = next(model.parameters()).device\r\n",
        "\r\n",
        "\t\tself.activations_map = {}\r\n",
        "\t\tself.gradients_map = {}\r\n",
        "\r\n",
        "\t\tself.model.eval()\r\n",
        "\t\tself.register_hooks()\r\n",
        "\r\n",
        "\tdef register_hooks(self):\r\n",
        "\t\tdef _wrap_forward_hook(layer_name):\r\n",
        "\t\t\tdef _forward_hook(module, input, output):\r\n",
        "\t\t\t\tself.activations_map[layer_name] = output.detach()\r\n",
        "\t\t\treturn _forward_hook\r\n",
        "\r\n",
        "\t\tdef _wrap_backward_hook(layer_name):\r\n",
        "\t\t\tdef _backward_hook(module, grad_out, grad_in):\r\n",
        "\t\t\t\tself.gradients_map[layer_name] = grad_out[0].detach()\r\n",
        "\t\t\treturn _backward_hook\r\n",
        "\r\n",
        "\t\tfor name, module in self.model.named_modules():\r\n",
        "\t\t\tif name in self.target_layers:\r\n",
        "\t\t\t\tmodule.register_forward_hook(_wrap_forward_hook(name))\r\n",
        "\t\t\t\tmodule.register_backward_hook(_wrap_backward_hook(name))\r\n",
        "\r\n",
        "\tdef make_one_hots(self, target_class=None):\r\n",
        "\t\tone_hots = torch.zeros_like(self.output)\r\n",
        "\t\tif target_class:\r\n",
        "\t\t\tids = torch.LongTensor([[target_class]] * self.batch_size).to(self.device)\r\n",
        "\t\t\tone_hots.scatter_(1,ids,1.0)\r\n",
        "\t\telse:\r\n",
        "\t\t\tone_hots = torch.zeros((self.batch_size, self.num_classes)).to(self.device)\r\n",
        "\t\t\tfor i in range(len(self.pred)):\r\n",
        "\t\t\t  one_hots[i][self.pred[i][0]] = 1.0\r\n",
        "\t\treturn one_hots\r\n",
        "\r\n",
        "\tdef forward(self, data):\r\n",
        "\t\tself.batch_size, self.img_ch, self.img_h, self.img_w = data.shape\r\n",
        "\t\tdata = data.to(self.device)\r\n",
        "\t\tself.output = self.model(data)\r\n",
        "\t\tself.pred = self.output.argmax(dim=1, keepdim=True)\r\n",
        "\r\n",
        "\tdef backward(self, target_class=None):\r\n",
        "\t\tone_hots = self.make_one_hots(target_class)\r\n",
        "\t\tself.model.zero_grad()\r\n",
        "\t\tself.output.backward(gradient=one_hots, retain_graph=True)\r\n",
        "\r\n",
        "\tdef __call__(self, data, target_layers, target_class=None):\r\n",
        "\t\tself.forward(data)\r\n",
        "\t\tself.backward(target_class)\r\n",
        "\r\n",
        "\t\toutput = self.output\r\n",
        "\t\tsaliency_maps = {}\r\n",
        "\t\tfor target_layer in target_layers:\r\n",
        "\t\t\tactivations = self.activations_map[target_layer]\t#[64, 512, 4, 4]\r\n",
        "\t\t\tgrads = self.gradients_map[target_layer]\t#[64, 512, 4, 4]\r\n",
        "\t\t\tweights = F.adaptive_avg_pool2d(grads, 1)\t#[64, 512, 1, 1]\r\n",
        "\r\n",
        "\t\t\tsaliency_map = torch.mul(activations, weights).sum(dim=1, keepdim=True)\t\r\n",
        "\t\t\tsaliency_map = F.relu(saliency_map)\t#[64,1,4,4]\r\n",
        "\t\t\tsaliency_map = F.interpolate(saliency_map, (self.img_h, self.img_w),\r\n",
        "\t\t\t\tmode=\"bilinear\", align_corners=False)\t#[64,1,32,32]\r\n",
        "\r\n",
        "\t\t\tsaliency_map = saliency_map.view(self.batch_size, -1)\r\n",
        "\t\t\tsaliency_map -= saliency_map.min(dim=1, keepdim=True)[0]\r\n",
        "\t\t\tsaliency_map /= saliency_map.max(dim=1, keepdim=True)[0]\r\n",
        "\t\t\tsaliency_map = saliency_map.view(self.batch_size, 1,\r\n",
        "\t\t\t\t\t\t\t\t\t\t\tself.img_h, self.img_w)\r\n",
        "\t\t\tsaliency_maps[target_layer] = saliency_map\r\n",
        "\r\n",
        "\t\treturn saliency_maps, self.pred"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}